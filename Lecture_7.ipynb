{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "The art of having the system figuring out correct behavior by itself rather than teaching it actively, maybe cause we don't know the correct answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There was not real progress within this field until ~2 years ago, when Alpha Go beat the world champion of Go with the help of reinforcement learning:\n",
    "- The system started without any real knowledge of the game, and simply played copies of itself until it became a master, <b>without ever playing against humans</b>.\n",
    "- The real interesting thing about Go is that there are soooooo many possible boards-states that it would be literally impossible to train by just sampling states (there are more states than the number of atoms in the universe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Basics of reinforcement learning:\n",
    "\n",
    "The learning system is referred to as an <i>agent</i>. The agent observes the state (feature vector) of the environment, and based on it, takes an <i>action</i> (output) on the environment. The effect of the action on the environment produces a new state, which is passed to the agent, and-so-on. Eventually, you will have reached an end-state (such as winning or losing a board-game), which will produce a <i>rewards</i> (whether positive or negative its still referred to as a reward).\n",
    "\n",
    "#### Differences to other methods:\n",
    "- Supervised learning\n",
    "    - Time!\n",
    "    - Feedback is given as a scalar rewards, not the action you should take!\n",
    "    - Feedback is not immediate, but rather given after many actions.\n",
    "    - Can become better than system designer!\n",
    "- Control Theory\n",
    "    - No physical model of the world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discretization\n",
    "The world must be able to be discretizised into different states! Each state must be able to be uniquely described.\n",
    "\n",
    "#### Policy\n",
    "The policy defines which action to take in each state. Can be represented as a look-up table (state1 leads to action1, state2 to action2, and so on...)\n",
    "\n",
    "#### Rewards\n",
    "$r(x, a)$ the rewards for making action $a$ in state $x$.\n",
    "\n",
    "---\n",
    "\n",
    "### Reinforcement Learning Goal\n",
    "Finding an optimal policy such that the actions produces as much reward as possible. To do this we have to solve the following problems:\n",
    "- How to evaluate the current policy?\n",
    "- How to know which policies to explore?\n",
    "\n",
    "#### Value function\n",
    "A function to evaluate how good a policy is.\n",
    "1. $V(s_i) = \\sum_{k=0}^{\\inf}r_{t+k}$ which sums of all future rewards.\n",
    "2. $V(s_i) = \\sum_{k=0}^{\\inf}\\gamma^kr_{t+k}$ where $0 \\leq \\gamma \\leq 1$, which sums all rewards from the nearby future, putting greater weight on closest states.\n",
    "\n",
    "Note that the optimal value function is unknown for each policy when we start, and we need to learn them.\n",
    "\n",
    "#### Accumulated Reward\n",
    "What if we can move to one state in two different fashions from our current state? How do we calculate our reward for that state?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### How do we learn the Value Function?\n",
    "\n",
    "#### Credit Assignment Problem\n",
    "What singular action brought about the greatest contribution to victory? One possible solution is to give a reward based on distance to positive end-state.\n",
    "\n",
    "#### 1. Monte Carlo Approach\n",
    "Generate many episodes (paths) through the state-space ending in different states. After a reward $r$, update the value functions of the visited states:\n",
    "\n",
    "$\\vec{V}(s_k) \\leftarrow (1-\\eta)\\vec{V}(s_k) + \\eta\\gamma^mr$\n",
    "\n",
    "This requires <b>a lot</b> of memory.\n",
    "\n",
    "<b>EXAMPLE:</b> Consider you are moving along a cliff, where if the agent moves close to the cliff-side he might fall down, resulting in rewards -100. Reaching the goal is +100. There are two different policies: prioritize moving quickly to the goal ignoring distance to cliff, or maximize distance to cliff first and the move. Moving just a small distance away from the cliff would result in the Reward values for almost all policy2-states to be better than policy1-states.\n",
    "\n",
    "#### 2. Temporal Difference Approach\n",
    "Update the Value Function V after each state transition.\n",
    "\n",
    "$\\vec{V}(s_k) \\leftarrow (1 - \\eta)\\vec{V}(s_k) + \\eta(r_k + \\gamma\\vec{V}(s_k + 1))$\n",
    "\n",
    "<b>EXAMPLE:</b> Imagine a simple net. Calculate reward of state closest to end. Look-back at all ways to get there and update rewards one at a time. You will run into the same state multiple times, just update the reward each time.\n",
    "\n",
    "#### SUMMARY of learning Value Function\n",
    "- For a given policy, the value of each state is unknown before we learn it, and learns it by interacting with the environment\n",
    "- V(s) is found iteratively, starting for example with all values at 0, using the Monte Carlo or Temporal Difference Approach to update values.\n",
    "- The Temporal Difference method generally converges much faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Ultimate Goal: Find Best Policy\n",
    "How?\n",
    "\n",
    "- Brute force: test all possible policies and choose the one with the best value function.\n",
    "- Focus the search more on policies that seem promising, i.e. variations of policies that have already been found to give good value functions.\n",
    "\n",
    "#### Exploration-Exploitation Dilemma\n",
    "How much should we explore new policies and how much should we exploit what we already learned? Solution:\n",
    "\n",
    "#### Q-Function\n",
    "An additional dimension to answer the question \"which policies should we explore?\".\n",
    "- Let V*(s) denote the value function of the optimal policy\n",
    "- Q(s,a) is expected future reward of doing action a in state s and the following the optimal policy:\n",
    "\n",
    "$$Q(s_k, a) = r(s_k, a) + \\gamma{}V^*(s_{k+1})$$\n",
    "\n",
    "- Q(s,a) indirectly encodes the optimal policy and its value function\n",
    "- Q must be learned!\n",
    "\n",
    "So in the end, Q will be a giant table where for each state, there is 1 reward value per action possible, so the agent can either pick the best, or 1 of top10, and so on, to get a good exploration going.\n",
    "\n",
    "#### Q-Learning\n",
    "\n",
    "$\\hat{Q}(s_k, a_j) \\leftarrow (1-\\eta)\\hat{Q}(s_k, a_j) + \\eta(r + \\gamma{}\\hat{V}(s_{k+1})) = (1-\\eta)\\hat{Q}(s_k, a_j) + \\eta(r + \\gamma\\max_a\\hat{Q}(s_{k+1},a))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialize lookup-table for Q(s, a) with random values\n",
    "for each episode:\n",
    "    init a start state s\n",
    "    repeat for each step k in the episode:\n",
    "        choose an action aj\n",
    "        take action aj and observe reward r and next state s(k+1)\n",
    "        update estimated Q(s,a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### $\\epsilon$-greedy exploration\n",
    "- Make a random action with probability $\\epsilon$\n",
    "- May want to explore more in the beginning (large $\\epsilon$) of the training phase and less towards the end\n",
    "\n",
    "---\n",
    "\n",
    "### Parameters\n",
    "\n",
    "- <b>$\\eta$</b> Learning rate\n",
    "- <b>$\\gamma$</b> Discount factor. High number means all rewards matter, even those far away. Low number only takes close rewards into account\n",
    "- <b>$\\epsilon$</b> Exploration factor. The chance of the agent taken an action which isn't the learned optimal."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:TensorFlow]",
   "language": "python",
   "name": "conda-env-TensorFlow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
