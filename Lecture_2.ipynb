{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning - Linear Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap - Supervised Learning\n",
    "- Task: Learn to predict/classify new data based on labelled examples\n",
    "- Input: Set of examples with features vectors x with label $\\Omega$. Today we assume $\\Omega \\in \\{-1,1\\}$.\n",
    "- Output: A function $sign[f(x;w_1,...,w_k)] \\rightarrow \\Omega$\n",
    "\n",
    "Machine learning needs to find the function $f$ and ajust the parameters $w$ so <u>new</u> vectors are classified correctly.\n",
    "\n",
    "> $sign[x]$ gives -1 if x is negative and 1 if x is positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantages of using a parametric function $f(x;w_1,...,w_k)$\n",
    "\n",
    "- Only stores a few parameters instead of all training samples, like in k-NN.\n",
    "- Fast to evaluate on which side of the line a new sample is on, for each example $w^{T}x < 0$ or $w^{T}x > 0$ for binary case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model of a Neuron\n",
    "\n",
    "Input signals $x_1, ..., x_n$ are each weighted by $w_1, ..., w_n$ before being summed before being fed to an Activation Function which determines the \"activity\" of the neuron (0, 1, or something inbetween).\n",
    "\n",
    "This basic model is called a \"Perceptron\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron\n",
    "\n",
    "$f(x_1,...,x_n;w_0,w_1,...,w_n) = \\sigma(w_0 + \\sum_{i=1}^{n}(w_{i}x_i)) = \\sigma(w_0 + \\vec{w}^{T}\\vec{x})$\n",
    "\n",
    "> Here, $\\sigma$ is the activation function, in this case the signum function\n",
    "\n",
    "Better formulation: add an $x_0 = 1$ so we can include the bias weight $w_0$ in the formula proper:\n",
    "\n",
    "$f(1,x_1,...,x_n;w_0,w_1,...,w_n) = \\sigma(\\vec{w}^{T}\\vec{x})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a 2-dimensional feature-space, we can think of a perceptron as drawing a line defined by the vector $(w_1,w_2)^T$ (being orthogonal to the line) bisecting the plane a distance $w_0$ from origo. At this line, the discriminant function $f$ will be 0, on one side positive and the other negative.\n",
    "\n",
    "## Task of Machine Learning: FIND THIS LINE\n",
    "Several different lines might work for training data, BUT we want it to generalize properly, meaning we need to think about which line is the most robust, <u>true</u> separating line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the best separator - optimization!\n",
    "Minimize or maximize a cost function $\\epsilon(w_0,w_1,...,w_n)$ with the weights as parameters. Think of the graph from last lecture!\n",
    "\n",
    "Ways to optimize:\n",
    "- Algebraic: Set derivative $\\frac{\\partial\\epsilon}{\\partial{}w_i} = 0$ and solve.\n",
    "- Brute force: Try many values systematically\n",
    "- Iterative: Follow the gradient direction until direction until minimum/maximum of $\\epsilon$ is reached. (in this case, you only need to know the local gradients, rather than global gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent/Ascent\n",
    "$$\\nabla\\epsilon = \\frac{\\partial\\epsilon}{\\partial\\vec{w}} =\\binom{\\frac{\\partial\\epsilon}{\\partial{}w_1}}{\\frac{\\partial\\epsilon}{\\partial{}w_2}}$$\n",
    "\n",
    "$\\vec{w}^{(t+1)} \\leftarrow \\vec{w}^{(t)} \\pm{} \\eta$ $\\frac{\\partial\\epsilon}{\\partial{}\\vec{w}}|_{\\vec{w}^{(t)}}$\n",
    "\n",
    "> $|_{\\vec{w}^{(t)}}$ means it only needs to be solved for $\\vec{w}^{(t)}$\n",
    "\n",
    "#### Choosing Step Length $\\eta$\n",
    "- Too small: will take too much time\n",
    "- Too large: will overshoot optimum.\n",
    "\n",
    "Gradient descent is not guaranteed to find the global optimum!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of Loss Functions \n",
    "\n",
    "#### Empirical Risk\n",
    "\n",
    "$\\epsilon(\\vec{w}) = \\sum_{i=1}^{N}I(f(\\vec{x}_{i};\\vec{w}) \\neq y_i)$\n",
    "\n",
    "This means \"sum the number of errounous dots\". The problem with this function is that it flips discretely between integer numbers (can't have float number of errors), meaning it can't be differentiated!\n",
    "\n",
    "#### Maximum margin\n",
    "Used in support vector machines.\n",
    "\n",
    "#### Square Error\n",
    "Used in neural nerworks (the most common cost function).\n",
    "\n",
    "$\\epsilon(\\vec{w}) = \\sum_{i=1}^{N}(\\vec{w}^{T}\\vec{x}_i - y_i)^2$\n",
    "\n",
    "where $N$ is number of training samples.\n",
    "\n",
    "$\\frac{\\partial\\epsilon}{\\partial\\vec{w}} = 2\\sum_{i=1}^{N}(\\vec{w}^{T}\\vec{x}_i - y_i)\\vec{x}_i$\n",
    "\n",
    "Gradient descent:\n",
    "\n",
    "$\\vec{w}^{(t+1)} = \\vec{w}^{(t)} - \\eta$ $\\frac{\\partial\\epsilon}{\\partial{}\\vec{w}} = \\vec{w}_t - \\eta{}\\sum_{i=1}^{N}(\\vec{w}^{T}\\vec{x}_i - y_i)\\vec{x}_i$ (Eq. 1)\n",
    "\n",
    "Algorithm:\n",
    "1. Start with a random $\\vec{w}$\n",
    "2. Iterate Eq. 1 until convergence.\n",
    "\n",
    "> <b>GRADIENT DESCENT WITH SQUARE ERROR WORKS POORLY WHEN:</b>\n",
    "> - The data is unevenly distributed. Values far away will \"draw\" the border towards them.\n",
    "> - The data contains outliers. Outliers will shift the line in the wrong direction.\n",
    "> - There are deep local minima, which often happens with outliers. (can be mitigated by having several different starting-points and comparing their respective optimum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines (SVM)\n",
    "Based on following ideas:\n",
    "- Only the data-points closest to the border are needed to define the border. These points are called <b>support vectors</b>.\n",
    "- Distance between the separating line and the support vectors should be as large as possible: choose $\\vec{w}$ so that it maximizes the margin $\\epsilon$ to closest support vectors! This should minimizr the risk of miss-classifying new datapoints.\n",
    "\n",
    "### Cost function\n",
    "$\\vec{w}^{T}\\vec{x}_s + w_0 = 1$\n",
    "\n",
    "$\\vec{x}_{s} = \\vec{x}_p + \\epsilon|\\vec{w}|$\n",
    "\n",
    "$\\vec{x}_s$ is a support vector and $\\vec{x}_p$ is a projection of the support vector on the line $\\vec{w}$.\n",
    "\n",
    "$=> \\epsilon(\\vec{w}) = 1/||\\vec{w}||$\n",
    "\n",
    "meaning epsilon should be as large as possible, meaning we want a small value of the norm of $\\vec{w}$. Maximizing the margin is the same as minimizing the norm of $\\vec{w}$. AND no training samples must reside within the margin region (which leads to difficult optimization outside the scope of this course):\n",
    "\n",
    "$\\min||\\vec{w}||^2$\n",
    "\n",
    "subject to $y_{i}(\\vec{w}^{T}\\vec{x}_i + w_0) \\geq 1$\n",
    "\n",
    "#### Examples\n",
    "- Not centered line: Steep w, meaning small $\\epsilon$.\n",
    "- Properly centered: Flatter function, meaning larger $\\epsilon$.\n",
    "- Properly tilted line: Even smaller w, meaning even larger $\\epsilon$.\n",
    "\n",
    "### Soft Margin\n",
    "- Outliers may mean you get a very narrow $\\epsilon$.\n",
    "- Data may not actually be linearly separable at all!\n",
    "\n",
    "To solve this, we can allow datapoints to be on the wrong side of their margin, with a slack variable $\\xi_i$, so the new loss-function would be:\n",
    "\n",
    "$\\min||\\vec{w}||^2 + C\\sum{\\xi_i}$\n",
    "\n",
    "subject to $y_{i}(\\vec{w}^{T}\\vec{x}_i + w_0) \\geq 1 - \\xi_i$\n",
    "\n",
    "where $C$ is a user-defined hyper-parameter signifying the trade-off: a small $C$ means we are lenient in allowing outliers, but a large C means the contribute a lot to the loss, and we might as well do non-soft margin. Most often, $C$ is chosen by iterating through different values of it ($2^{-5}, 2^{-3}, ..., 2^{15}$) during cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:TensorFlow]",
   "language": "python",
   "name": "conda-env-TensorFlow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
