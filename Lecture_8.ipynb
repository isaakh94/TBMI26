{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning - Dimensionality Reduction, Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: Clustering: In supervised learning, we know which class which point in the training set belongs to. In unsupervised learning, we need to infer this as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Task:</b> Find underlying structure in data\n",
    "\n",
    "<b>Input:</b> Training data examples <i>without labels</i>\n",
    "\n",
    "<b>Output:</b> Data described in a simpler or more informative way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics\n",
    "As earlier, we minimize a cost function. It could be to <i>maximize the variance</i> of the data (PCA), or to <i>maximize class separability</i> (LDA).\n",
    "\n",
    "This will result in finding a new representation of the data.\n",
    "\n",
    "### Applications\n",
    "- Feature Extraction - finding order or structure in data\n",
    "- Dimensionality reduction - keeping only the \"important\" parts of the signal. Too many dimensions lead to:\n",
    "    - More noise\n",
    "    - More parameters needed\n",
    "        - More local optima\n",
    "        - Poorer generalization\n",
    "        - Higher computational effort\n",
    "    - Difficult to visualize data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## PCA\n",
    "Principal Component Analysis\n",
    "\n",
    "The purpose of PCA is to find the two principal components which describes the directions of maximum variation in the data.\n",
    "\n",
    "The variance in direction $\\hat{w}$: (suppose $\\vec{x}$ has mean 0)\n",
    "\n",
    "$\\sigma_\\hat{w}^2 = E[(\\vec{x}^T\\hat{w})^2] = E[(\\hat{w}^T\\vec{x})(\\vec{x}^T\\hat{w})] = \\hat{w}^T E[\\vec{x}\\vec{x}^T]\\hat{w} = \\hat{w}^TC\\hat{w} = \\frac{\\vec{w}^TC\\vec{w}}{\\vec{w}^T\\vec{w}}$\n",
    "\n",
    "where $C$ is the covariance matrix of $\\vec{x}$\n",
    "\n",
    "So, we differentiate this expression:\n",
    "\n",
    "$\\frac{2}{\\vec{w}^T\\vec{w}}(C\\vec{w} - \\sigma^2_\\hat{w}\\vec{w}) = 0$\n",
    "\n",
    "$C\\vec{w} = \\sigma^2_\\hat{w}\\vec{w}$\n",
    "\n",
    "Which is an eigenvalue-decomposition!\n",
    "\n",
    "#### Limitations of PCA\n",
    "Variance is not always the most important goal! For instance: consider two parallell lines of points! If we project them on the main component, they will be completely muddled, but if we project on the secondary component, we'll get two nice distributions!\n",
    "\n",
    "Transformation for optimal separation of classes is different from PCA!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## LDA\n",
    "Linear Discrimination Analysis\n",
    "\n",
    "We want to minimize the variance and maximize the distance:\n",
    "\n",
    "$\\epsilon(\\vec{w}) = \\frac{(\\mu_1-\\mu_2)^2}{\\sigma_1^2+\\sigma_2^2} = \\frac{\\vec{w}^TM\\vec{w}}{\\vec{w}^TC_{tot}\\vec{w}}$\n",
    "\n",
    "$\\vec{w} \\tilde{} C^{-1}_{tot}(\\vec{x}_1-\\vec{x}_2)$ (tilde means proportional to)\n",
    "\n",
    "#### LDA as a classifier\n",
    "Easy to use! Test it before moving on to complicated stuff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Categorization\n",
    "Categorization and grouping of objects based on similar properties is an important functionality in learning and knowledge representation.\n",
    "\n",
    "This is often referred to as <b>clustering</b>.\n",
    "\n",
    "But what defines a cluster? Is it how the points are connected? Distance to other points? No clear answer.\n",
    "\n",
    "### k-Means clustering\n",
    "- Assume k clusters\n",
    "- Represent each cluster with a mean prototype vector $\\vec{p}_j$ at the cluster center\n",
    "- A datapoint belongs to the cluster with the closest prototype vector.\n",
    "\n",
    "How to:\n",
    "1. Start with k random prototpype vectors\n",
    "2. Iterate:\n",
    "    1. Assignment: Assign each data vector to the closest prototype cector. Denote the set of data vectors assigned to cluster P by S.\n",
    "    2. Update all prototype vectors to the mean of the clusters.\n",
    "    \n",
    "The function for assignment is not differentiable, so we cannot do gradient descent...\n",
    "\n",
    "### k-Means clustering is a case of Expectation Maximization\n",
    "Expectation Maximization can be used when we want to estimate model parameters but for each sample there is a hidden/missing parameter (like the class label)\n",
    "\n",
    "### Summary of Clustering\n",
    "- Not good to have to choose k manually\n",
    "- Different initializations give different results\n",
    "- May converge to degenerate solutions (empty clusters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:TensorFlow]",
   "language": "python",
   "name": "conda-env-TensorFlow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
