{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEEP NETWORKS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### INFORMATION ON LAB 3:\n",
    "> Liu-IT fucked up, so labs might become more \"manual\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <b>ImageNet</b> dataset consists of 14M images, where 10M are annotated, and 1M lack a bounding box. When evaluating this set, systems get 5 guesses as to what the image pictures.\n",
    "\n",
    "Human error rate on this set is circa 5%, but is only so high because of the many different breeds of dogs in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Loss Functions\n",
    "### Motivation: maximum likelihood estimation\n",
    "$\\theta = arg\\min_\\theta\\sum_{i=1}^{N}(x^{(i)} - \\theta)^2$\n",
    "\n",
    "In supervised learning, we learn a conditional probability distribution over target  values $y$, given features $x$:\n",
    "\n",
    "$\\theta = arg\\max_\\theta\\sum_{i=1}^{N}\\log{}P(y^{(i)}|x^{(i)};\\theta)$\n",
    "\n",
    "which is the same as minmizing cross-entropy $E_X[-\\log{}P(Y|X)]$\n",
    "\n",
    "### Cross-entropy cost function\n",
    "$C = -(y_i\\ln{}h(\\vec{x}_i) + (1 - y_i)\\ln{}(1 - h(\\vec{x}_i))$ where $y \\in {0, 1}$\n",
    "\n",
    "The sigmoid and cross-entropy balance each other, meaning the derivative of the cost (such as when doing gradient descent) becomes: $f(z) - y_i$\n",
    "\n",
    "Very convenient! This means the gradient will backpropagate with increasing strength, meaning it will never vanish! <i>In the final layer, you should combine the appropriate cost function with the appropriate activation function!</i>.\n",
    "\n",
    "With softmax: $-\\sum_n\\sum_ky_{kn}\\ln{}h_k(\\vec{x}_n)$\n",
    "\n",
    "### Surrogate Loss Function\n",
    "Example: 0-1 loss for class membership (one-hot coding) surrogated/replaced with log-likelyhood (cross-entropy).\n",
    "- Do this so data is continually differentiable\n",
    "- Training vs Testing might be skewed with 0-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Handwritten digit recognition\n",
    "The MNIST database is a bunch of 28x28 pixels large images of hand-written numbers, which need to be classified.\n",
    "\n",
    "#### Example of implementation of network to solve this\n",
    "- Translate each image to a vector $\\vec{x}$ with 1+28x28 components (greyscale values). The greyscale value is a fraction betweem 0 (black) and 1 (white). The 1+ is the bias factor.\n",
    "- Feed the image to the network with a softmax final layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Regularization\n",
    "We can regularise the training of a network by adding an additional term to the error function somehow:\n",
    "- Give preference to parameter vectors with smaller Euclidian norms (lengths)\n",
    "- Give preference to parameter vectors with smaller absolute-value norms\n",
    "\n",
    "#### Selected regularization techniques\n",
    "- <b>Dataset Augmentation:</b> Generate new training data by systematically transforming the existing data (rotation and scaling)\n",
    "- <b>Early Stopping:</b> Stop the training when the validation set error goes up and backtrack to the previous set of parameters.\n",
    "- <b>Baggin/ensemble methods:</b> Traing several different models separately, then have all the models vote on the output.\n",
    "- <u><b>Dropout:</b></u> Randomly set a fraction of units to zero during training. This means you will get redundancy in your net: several paths will lead to same answer, which will increase robustness.\n",
    "\n",
    "#### Local Minima\n",
    "Does regularization actually matter if we still end up in local minima because of the gradient descent? YES! What matters is not if we find THE BEST model, its if we find a GOOD MODEL. Sometimes though, a local minima may be really bad. We can solve this by looking at the gradient during training: if it approaches 0 and the performance is still bad, we've ended up in a local minima.\n",
    "\n",
    "Local mimina are actually quite rare however, saddle points are more common. We can get out of these by looking at the second-derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Initialization and Normalization\n",
    "#### Initialization\n",
    "Gradient descent based learning might lead to wrong directions, and long training  times because of non-optimized step-lengths. These problems are usually adressed by initialising in different random ways:\n",
    "- Initial weights should not be symmetric or identical, which will lead to forced redundancy\n",
    "- Initial values should be large to break symmetry\n",
    "- Initial values should not be too large so as to avoid numerical issues (overflow).\n",
    "\n",
    "Good heuristic for initialization: draw weights from Gaussian or uniform distribution.\n",
    "\n",
    "There are many different ways of initialization. Most often, the only good way of finding a good one is trying them out!\n",
    "\n",
    "#### Batch Normalization\n",
    "In deep networks, the simultaneous update of layers will have second, third, ... order effects. To reduce this effect, $\\vec{z}$ is replaced during learning with a weird formula calculated on mini-batches.\n",
    "\n",
    "If you have small batch sizes (less than 16 to 30), this will severely mess it up.\n",
    "\n",
    "#### Minibatch methods\n",
    "- BATCH: deterministic, whole dataset\n",
    "- STOCHASTIC GRADIENT DESCENT: single training sample at a time (wildly unstable)\n",
    "- MINIBATCH: subsets from training set\n",
    "    - Large enough to exploit multicore architecture\n",
    "    - Size coincides with accuracy of gradients\n",
    "    - Small enough to fit in memory\n",
    "    - Often power of 2\n",
    "    - Number of minibatches coincides with regularization\n",
    "    - Repeated drawing: <i>epochs</i>\n",
    "    \n",
    "#### Learning Rate, $\\alpha$\n",
    "Stochastic methods require decaying rate. Sum of learning rates goes to infinity. As a rule of thumb, the initial rate should be larger than what initial results suggests, and final rate should be about 1%.\n",
    "\n",
    "<b>Momentum</b>\n",
    "\n",
    "Gradient is problematic due to curvature and noise, so typically you introduce velocity as exponentially decaying moving average of gradients. ADAM is a method of gradient velocity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Network Architectures\n",
    "#### LeNet 5\n",
    "Input -> Convolution -> subsampling -> convolutoins -> subsampling -> full connection -> full connection -> gaussian connections -> output\n",
    "\n",
    "Used average pooling and sigmoid/tanh activation.\n",
    "\n",
    "#### Deep Neural Networks (simple)\n",
    "Input -> (conv -> pooling ->) x 3 -> locally connected layer -> (fully connected ->) x 2 -> output\n",
    "\n",
    "#### AlexNet (55% acc, 3M operations)\n",
    "Uses max-pooling, ReLU activation, and dropout regularization\n",
    "\n",
    "#### ResNet (69% acc, 4M operations)\n",
    "152 layers!!! Works by having <i>shortcuts</i>! Meaning the input to one layer is also directly connected to the input of the next layer, which results in error propagating backwards very quickly, acting like a much shallower network.\n",
    "\n",
    "#### Inception v4 (80% acc, 17M operations)\n",
    "EXTREMELY COMPLICATED. Vaguely based on ResNet: possesses repeated subgraph based on ResNet topology. There are also \"reduction\" subgraphs and pooling layers.\n",
    "\n",
    "NOTE that these accuracies are based in image recognition, and for other stuff ResNet is still king!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:TensorFlow]",
   "language": "python",
   "name": "conda-env-TensorFlow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
