{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble/committee of different <i>base classifiers</i> vote on outcome.\n",
    "\n",
    "Random Forest, for instance, is a bunch of Random Decicion Trees which work as an ensemble when predicting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap AGGregatING (BAGGING)\n",
    "Train each base-classifier using a subset of the training data (this is how you train Random Forests).\n",
    "\n",
    "Complex base-classifiers may draw complex decision-boundaries, and be prone to over-fitting. This training method leads to reduced variance, and therefore a <u>reduced risk of overfitting</u>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "Train each base-classifier using all training data but with weights indicating how important each training sample is. Assign a different set of weights to the different base-classifiers. The assigning of weights might be serial, with the last classifier determining the weights for the next, or parallell, which each set of weights independent from the next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple/Weak Classifiers\n",
    "The components used in Ensemble Learning.\n",
    "\n",
    "Basically a \"rule-of-thumb\" classifier. For example: select only one feature to base the classification-threshold on (a decision-tree branch).\n",
    "\n",
    "### Decision Stump\n",
    "If you combine weak classifiers into a branch shape, it's called a decision stump:\n",
    "\n",
    "`  \n",
    "...A or B\n",
    "    /  \\\n",
    "   /    \\\n",
    "  -1    +1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification and Regression Trees (CART)\n",
    "Chaining decision stumps to each other.\n",
    "\n",
    "Leads to a piece-wise flat classification-function.\n",
    "\n",
    "> #### Regression Tree\n",
    "> Same thing, but all leafs end in real-valued output rather than a label.\n",
    "\n",
    "Of course, these trees can become bery complicated, which makes them prone to over-fitting.\n",
    "\n",
    "### Random Forest\n",
    "Bagging + Decision Trees\n",
    "\n",
    "For each tree, use a random subset of training samples. For each branching, use a random subset of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Boosting Algorithm\n",
    "Train weak classifiers sequentially!\n",
    "\n",
    "1. Set each example weight $d_i=1/N$ => $\\vec{d}_1$.\n",
    "2. Train weak classifier using these weights.\n",
    "3. Increase and decrease weights for wrongly and correctly classified training samples respectively. (we want new classifiers to focus on what we did wrong last) => $\\vec{d}_2$\n",
    "4. Train weak classifier using these weights.\n",
    "5. Repeat X times.\n",
    "6. Weight all classifier <u>outputs</u> according to their general performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Decision Stump\n",
    "Find best split threshold $\\tau$!\n",
    "\n",
    "Optimize the cost function, which is the EMPIRICAL RISK FUNCTION (number of wrong counts), with each count multiplied by the weight associated with it.\n",
    "\n",
    "> Observe that the minimization function will always be <= 0.5, since we could just flip it otherwise.\n",
    "\n",
    "Since a weak classifier is so easy to optimize, we don't need gradient descent, we can just brute-force it, testing one for each example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete AdaBoost\n",
    "1. Find a weak classifier that minimizes the weighted classification error.\n",
    "2. Update the weights depending on error $e^{-\\alpha_i y_i h(y_i)}$\n",
    "    \n",
    "    $\\alpha_i = \\frac{1}{2}\\ln\\frac{1-\\epsilon_i}{\\epsilon_i}$\n",
    "3. Renormalize so that all weights sum up to 1.\n",
    "4. repeat 1-to-3 for each classifier.\n",
    "5. Final classifier is sum of all classifiers multiplied by their respective $\\alpha$.\n",
    "\n",
    "### Outlier Problem\n",
    "Outliers will gain weight exponentially each iteration until the classifier is <u>really</u> bad.\n",
    "\n",
    "How to deal with it:\n",
    "- Monitor weights\n",
    "- Weight trimming\n",
    "    - Maximum weight threshold\n",
    "    - Disregard samples with large weight\n",
    "- Use alternative weight update schemes with less aggressive increase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Ensemble Learning\n",
    "- Nonlinear\n",
    "- Easy to use, just a few parameters\n",
    "- Inherent feature selection\n",
    "- Slow to train, but fast to classify\n",
    "- Look out for outliers: may cause issues!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Object Detection\n",
    "Detecting faces\n",
    "- Sweep a sub-window over the image, for each position, ask yes/no is there a face here.\n",
    "- Features: <b>Haar Features</b>: rectangular shapes with different sizes and divided into black/white areas, which yeild 1 number when applied to an image section. $\\sum_y\\sum_xI_{x,y}H_{x,y}$, I is image, H is filter (filter only has values of 0 or 1)\n",
    "- In this example, throw hundreds of thousands of these at random points in the sub-window, generating one number per feature.\n",
    "- Train the detector with AdaBoost.\n",
    "    - Positive set: small windows with faces\n",
    "    - Negative set: no faces\n",
    "    - Apply filters to each image to get features from that image.\n",
    "    - Train as above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:TensorFlow]",
   "language": "python",
   "name": "conda-env-TensorFlow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
